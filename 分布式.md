[00 开篇词 掌握好学习路径，分布式系统原来如此简单 (lianglianglee.com)](https://learn.lianglianglee.com/专栏/深入浅出分布式技术原理/00 开篇词 掌握好学习路径，分布式系统原来如此简单.md)

#### 为什么要分布式？

即分布式解决了哪些问题于如何解决，**分布式系统由一组通过网络进行通信进行协调工作的多个计算机节点组成**

- 单机性能：解决了单机服务的性能瓶颈问题，通过多节点（**水平扩展**）提升系统处理请求与响应的能力，除此之外多节点还可以进行**负载均衡，**提高系统的吞吐量与响应速度
- 单机可用性：单机如果出现故障那么服务就会停止，分布式多个节点通过**增加冗余**，确保系统在某个节点不可用时任然可以提供服务，某个节点故障不会导致系统瘫痪
- 系统维护与迭代：分布式将单体系统分为**多个模块**，对每个模块可以独立开发部署于维护，而不用对整个系统重新编译，测试，提高系统迭代效率。且可以使用自动化管理工具对每个节点进行单独的监控于管理

#### 实现分布式计算需要考虑的问题

- 如何找到服务：系统中的不同服务如何找到对方（服务1如何找到服务2），一般使用**服务注册发现机制**
- 如何找到实例：找到服务后，该发给服务的哪个实例？，若服务的实例是无状态的，那么采用**负载均衡**策略就行（轮询，权重，hash，一致性hash等策略），若是有状态的，需要通过**路由服务**确定请求发到哪个实例
- 如何管理配置：分布式系统中有许多服务，每个服务还有多个实例，且还可能自动扩缩容，若使用配置文件效率太低，一般使用中心化的存储来统一管理配置，即**配置中心**
- 如何进行协同：单体系统中，功能模块之间的协同通过系统的锁就可以进行，但是在分布式系统中，不同的服务运行在不同的机器上，使用**分布式锁**来解决
- 如何保证请求只执行一次：各个模块之间通过网络连接，如果发生了网络抖动，可能会触发重试策略，导致同一个请求被执行多次，一般分布式系统使用**幂等操作**确保请求只执行一次
- 如何避免雪崩：分布式系统中由于一个部分出现故障而引发整个系统其他部分也出现故障，如服务的一个实例出现故障而下线，引发其他实例负载升高导致所有实例都出现问题，避免雪崩的思路有**快速失败**和**降级机制**（熔断、降级、限流等）或者**弹性扩容机制。**快速失败会导致部分请求失败
- 如何监控警告和故障恢复：需要清楚系统内部的状态，如接口的延迟和可用性能等，需要分布式追踪Trace等

#### 实现分布式存储需要考虑的问题

- 如何数据分片：单机不可能存下所有数据，因此需要解决如何将数据按一定的规则分别存储到不同的机器上，**hash分片**和**region分片**
- 如何进行数据复制（备份）：为了满足高可用，需要对数据冗余处理，一般进行中心化方案（主从复制、一致性协议如Raft和paxos），并需要了解表现出来的一致性级别（线性一致性、顺序一致性、最终一致性等）
- 分布式事务：于单机的事务区别在于如何获得事务的ID（以此在并发事务冲突时判断哪个事务成功与失败），单机系统中按照时间戳生成事务ID，多机分布式系统如果在机器物理距离短的情况（RTT短）中可以从整个系统中选一台机器节点，按照单机模式生成事务ID，其他节点都从该节点获取事务ID。但当全球分布式系统中RTT过高，可以使用GPS+原子钟的方式实现

#### 共性总结

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1722064561911-26951f1a-b819-4a07-a505-9b1a14432c45.png)

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1722064672608-dc180f84-b601-4a17-b597-a1fd97d44a96.png)

#### 实现分布式系统后带来的新问题

- 故障处理：**全部失败与部分失败**，分布式系统由多个计算机节点组成，虽然每个计算机节点都是全部失败的模型（系统故障后停止整个系统都停止工作，而不是给出错误的结果，保证一致性，但不能保证可用性），但是整个分布式系统为部分失败，即系统中某些节点出现了问题，整个系统不停止服务，而是只停止或回滚受影响的服务，因此**需要构建高可用的系统**，且若为网络分区问题，在网络分区回复后需要正确的处理数据一致性。应对**部分失败的策略有快速失败或降级机制等**
- 远程调用：在单机系统中各个模块之间的调用都是本地调用，而分布式系统中不同的模块运行在不同的机器上，需要通过网络来调用，因此会带来很大的复杂性，网络中的请求可能会延迟或者丢失，因此需要采用**超时重试机制**，同时重试时需要增加**幂等机制**，保证请求只会执行一次
- 全局时钟与多个时钟：在多主复制的情况下，A与B两个节点都是用自己本地的时钟，但是A的时间比B的时间要晚，当先修改了A节点上的数据，再修改B节点上的同一个数据时，可能导致A上数据的记录时间要比B上的数据的记录时间要晚，合并数据时就会选取A上的数据版本，但是实际上B的数据版本才是最新的，就会导致数据的顺序错误，因此不能依赖各个节点本地的时间进行事件排序，一般可以在分布式系统中使用一个专门的**事时服务器**所有节点从该服务器获取事件的发生时间
- 服务协同问题：某些时候在同一时刻只允许一个线程操作某个数据或者执行操作（控制多个线程对共享资源的访问），在单机中因为线程都运行在同一个进程中，可以使用锁、信号量（资源的多副本并发访问），而分布式系统是跨机器的多进程同步，可以使用一个单独的**同步服务器**作为同步操作的管理者，多个进程需要同步时需要到同步服务先获取锁再操作，其他进程必须等待。但是同步服务不能是单例的，需要多个实例，因此还需要使用**raft算法**实现多个实例的数据一致性

#### CAP理论

[CAP理论十二年回顾：”规则”变了_架构_Eric Brewer_InfoQ精选文章](https://www.infoq.cn/article/cap-twelve-years-later-how-the-rules-have-changed/)

[CAP定理一文带你速解（通俗易懂，图文并茂）-云社区-华为云](https://bbs.huaweicloud.com/blogs/416696)

任何基于网络的数据共享系统，最多只能满足一致性、可用性、分区容忍性三要素中的两个

C一致性：系统所有节点同一时间内备份的数据都是相同的，即写成功后所有节点读操作都必须能读到最新值，写失败所有节点写操作都读不到，**强调数据的正确性**

A可用性：系统在任何时候都可以响应请求，即使某些节点失效也能保证部分功能运行，**强调服务一定可以响应**

P分区容忍性：系统对于网络分区（系统某些节点之间没法相互通信，导致了数据不一致）仍然能提供服务，**保证系统间网络出现问题时系统可用**

**分析：一般会首先保证p（即节点间出现网络通信问题也会保证可用）**，保证P的情况下，如果节点间网络出现了问题，此时节点1插入了数据，节点2因为网络问题没法同步数据，节点1和节点2中数据不一致，如果需要保证C一致性那么节点2就应该停止服务，即不满足了A可用性，如果要保证A可用性，那么节点2继续提供服务就会导致数据不一致即无法满足C数据一致性。一般业务会保证AP，除非数据一致性要求很高的业务如金融产品

#### 注册发现

分布式系统按照资源（计算密集型、IO密集型等）和业务等维度对单体服务进行拆分。

- 不同的模块之间如何相互调用：使用**REST API**（通过http协议）或者**RPC**进行跨服务的调用，都需要知道被调用服务的IP与PORT
- 如何获取被调用服务的port与IP（即注册发现）：

- - 最容易想到的是在服务调用方配置IP与port列表，但是当有多个服务时对每个服务都需要维护一个调用列表，是非常难以维护的。
  - 将配置IP与port改为配置域名与port，这样可以通过解析域名获得所有被调用服务B实例的IP列表，因此当B的实例有变更，只需要修改服务B的域名解析就可以了，而不用修改调用方的配置，但是当B的某个实例挂了，但是域名解析并不会变（并不能更新状态），依然会获得该实例的IP，导致访问出错
  - 因此最终，使用一个中间层，同时保证**注册发现**和**状态更新与通知，**使用可用性高、支持发布订阅的存储系统（如**zookeeper、etcd**）

- - - **服务注册：**服务的每个实例每间隔30s向存储中介上报一次IP与port信息，同时告诉中介该信息的有效期，如90s，若实例存活则会每30s向存储中介更新一次信息，若实例崩溃了，那么超过有效期那么实例的信息就会被存储中介删除，保证了数据同步的最终一致性
    - 状态更新与通知：服务的调用方采用**发布订阅**或**轮询**的方式通过存储中介获得被调用服务的状态信息，保证了数据同步的最终一致性

#### 负载均衡

调用方通过注册发现组件获取了被调用方多个实例的IP与端口，但是调用方应该将请求发向被调用服务的哪个实例？

根据被调用服务是否存在状态设计不同的负载均衡策略，有状态的服务就需要根据请求中的状态经请求路由到正确的实例中。

- 无状态的负载均衡：不需要关心请求的状态（用服务的哪个实例处理都可以，实例不在本地存储状态信息，状态信息会存到Mysql、Redis等存储器中）

- - 轮询：将请求按顺序依次分配给不同的实例（适用于负载能力差异较小的多个实例），并不利用请求的状态信息。
  - 权重轮询：给每个实例分配一个权重，分给实例的请求数量与权重成正比进行轮询（适用于服务的实例之间负载能力有差异的情况），并不利用请求的状态信息。

- 半状态的负载均衡：虽然负载均衡利用请求的状态信息进行路由，但是**仅仅使用简单的规则处理，****后端实例可以利用路由规则来进行优化（但后端并不是全状态）**，如Hash运算再取模。路由并不保证正确性，由后端保证正确性（如实例内保存一些缓存提升性能，但是如果被路由来的请求在实例中并没有对应的缓存，实例可以从中心存储Mysql等中读取需要的数据重建缓存，保证请求的正确性）。

- - Hash：将请求中的状态信息按照Hash算法并取模固定分配到一个实例上，不能解决实例之间负载能力的差异问题
  - 一致性Hash：hash中是按照实例的数量取模，一旦实例的数量发生了变化，那么请求的路由就全部会发生变化。一致性Hash中定义一个固定大小的Hash环，先根据实例的唯一标识计算其在环中的位置`hash( Node ID) %2^32`，接着对于每个请求也计算其在环上的位置，最后按照请求在环中位置顺时针遇到的第一个实例节点即为该请求路由的节点，当实例个数变化时只会有部分请求的路由会发生变化。

- - - 但是一致性hash会存在公平性的问题，即因为实例在环上不是均匀分布的，会导致节点间承担的请求比例不一样，通过增加虚拟实例节点，在 Hash 环中路由到虚拟实例的请求，会被路由到它的真实实例上，每个实例都生成多个虚拟节点
    - 且无法按照权重分配，通过控制每个实例生成虚拟节点的数量与权重成正比来解决

- - 全状态负载均衡：即后端是有状态的，负载均衡需要保证正确性

- - - 一般以路由服务的形式存在（**nginx，nginx负载均衡器可以从zookeeper中获取注册发现配置**），在路由服务中会存储服务实例与状态信息的映射，路由服务根据请求中的状态信息选择服务的实例

#### 配置中心

- 单体服务架构下一般将配置信息视为代码的一部分（**配置即代码**），工程师编辑好配置文件，将配置发布到服务所在的机器上，接下来程序会通过加载本地存储中的配置文件使配置生效。
- 分布式下如果将配置视为代码的一部分，会导致不同服务的配置文件出现在不同的代码仓库中，当需要查看多个服务的配置时需要在一个个仓库中查找，**效率低下。**其次每个实例都会加载自己本地的配置那么可能会出现实例之间的配置信息不同，即**实例间的配置可能不一致。**对于配置的修改操作也会变得**很冗余**，需要对每个实例都修改一遍，且没法回滚。
- 因此引入一个中间层即配置中心，其希望的功能为：

- - 能够统一的管理整个分布式系统中所有服务的配置信息
  - 配置中心中同一个服务的实例之间的配置应该保持一致，所有**实例通过网络去配置中心加载同一份配置文件**
  - 配置中中心可以高效的修改、发布、回滚配置，进行配置的版本管理，并将**修改同步给每个实例**

- **配置中心与注册发现很类似**，注册中心时服务实例上线时主动发送自己的Ip与port，下线时会过期删除。而配置中心是工程师主动写入的并不会设置过期时间。

- - 选择合适的存储系统：选择Eureka这种AP系统，希望即使分区也可以提供服务，且配置并不会经常修改，即使分区但是多个Eureka上的副本数据通常还是一样的，而不是选etcd与zookeeper这种CP的存储系统

- 如何做到配置信息的同步：
- ![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1722159770295-3f10ea0d-90a1-4365-b010-3d4b5731b6b1.png)

实例首次启动时去配置中心获取完整的配置信息，即**首次同步。**在实例运行的过程中如果配置被修改可以使用订阅发布或轮询的方式更新配置，即**变更同步，数据同步是最终一致性的。**

- **如何让配置同步确保强一致性？**

配置中心与实例之间的同步是最终一致性，但是有些场景需要保证强一致性，如何才能保证**配置同步**，可以采用类似**两阶段提交**的方式来解决该问题。即配置中心选择一个实例作为协调者A，协调者向所有更新配置的节点发送prepare消息，即配置信息，节点收到配置信息后更新配置，阻塞当前节点的读写操作，进入prepare状态并向协调者A发送同意同步，（在这一步为了一致性放弃了一定的可用性，prepare状态下的节点是不可用的），当协调者A收到了所有节点的同意同步的反馈，再向所有节点发送commit消息，节点收到后则应用配置并接收读写请求，否则发送rollback消息，节点收到后回滚配置。

#### 分布式锁

锁：用于控制并发，保证的多CPU多线程的环境中，某一时刻只有一个线程进入临界区代码，从而保证数据操作的一致性。

**单机中锁是控制一个进程中多个线程对共享资源的访问，分布式锁控制多个不同机器上的进程对中心存储上资源的访问，都是只有持有锁的进程或线程才可以执行临界区的代码。**

- 进程中的锁，由操作系统提供，本质上是使用一个存储在共享内存中的整数来表示锁的状态。0表示锁空闲，加锁时判断锁是否空闲再将锁状态改为1，解锁将锁的状态改为0，操作系统来保证加锁与解锁的原子性。
- 分布式中的锁也是一样的思路，通过一个状态来表示加锁与解锁，只是将该状态存放在所有不同主机上的实例都能访问到的位置，即放到存储服务中如Redis，再通过网络访问修改锁的状态。

分布式锁的特性：

- 互斥：同一时刻只有一个进程可以获得锁
- 超时机制：设置**超时时间**防止死锁，避免获得锁后实例挂了而导致无法解锁。同时要可以在有需要的情况**延长锁时间**，可以在获得锁的服务与锁节点之间**保持心跳**，每收到一次心心跳就延长一次超时时间，**同时解决延长与超时两个问题**
- 完备的锁接口：Lock**阻塞锁**与tryLock**非阻塞锁**
- 可重入性：已经持有锁的节点的线程可以再次成功获取锁，锁服务在处理加锁请求时记录当前获取锁的节点与线程的唯一表示，若下次还是已经持有锁的节点与线程获取锁那么直接返回加锁成功。
- 公平性：公平锁与非公平锁

#### 重试幂等

[该如何理解接口的幂等性？这里总结的很到位-阿里云开发者社区](https://developer.aliyun.com/article/1277688#:~:text=幂等实现的关键点 1 请求方生成唯一请求 Id ，服务提供方通过请求 Id 辨别请求是否重复，如果请求 Id,服务提供方根据请求参数经过一系列的 Hash 算法 生成对应的 Hash 值，若 Hash 值相同则判定为重复请求；)

在单机中模块之间的通信都是本地函数调用，要么进程整体崩溃，要么调用完成。而在分布式系统中模块之间通过网络调用，可能会出现请求调用失败的情况或者遇到网络延迟，无法判断远端的服务是否处理过了请求，而重试可能导致请求被多次处理，因此需要保证请求只被处理了一次。

- 重试加消息幂等：幂等即确保远端服务对请求处理一次和多次的结果是完全相同的，在重复的请求中每次都携带该请求不变的唯一ID，服务端在首到请求后先查询当前请求ID是否已经处理过，若处理过则不再重复执行相关逻辑。

- - **重试**加**幂等**带来的问题：在服务达到性能瓶颈时，某些节点的响应可能会因为负载过高而超时，此时在不断的请求重试可能会进一步的给节点带来压力造成雪崩效应，因此重试需要由限制，第一**重试次数限制**，第二**重试间隔采取退避策略**，每次重试的时间间隔都为上一次重试的两倍
  - 幂等性：

- - - 若请求本身就是幂等的，如查询数据或重置操作，并没有对状态修改，那么可以什么都不用做
    - 如何判断相等请求：

- - - - 非幂等的操作在请求中加入唯一ID，在处理请求时在服务端根据ID去重，确保相同ID的请求只处理了一次
      - 服务提供方根据请求参数经过一系列的 **Hash 算法**生成对应的 Hash 值，若 Hash 值相同则判定为重复请求；

- - - 实现方式

- - - - 利用**数据库特性**，将请求结果写入数据库时即表示请求已经处理，再次操作也无法成功（如金融账户，如数据库中已经有了该账户，则多次重复的注册请求也只能执行一次）
      - **Token机制**，生成请求的唯一ID，服务器通过唯一凭证保证同一个请求不会被执行两次。token的生成时机不能是在提交请求时否则每次请求的token都不一样了，如提交表单则应当是在进入表达页面的时候生成token，每次提交都携带相同的token，token的存储可能要用到第三方存储redis
      - **状态标识**，即主要思路就是通过状态标识的变更，保证业务中每个流程只会在对应的状态下执行，适用于**多流程的业务**（如一个订单要经历 创建订单 -> 订单支付/取消-> 发货-> 确认收货->关闭订单 这几个步骤），如果数据库中的标识已经进入下一个状态，这时候来了上一个状态的操作就不允许变更状态，保证了业务的幂等性。

- - - 外部系统的幂等性：若请求会影响外部系统的状态，如某个请求会调用外部服务发送EMail，需要外部服务提供保证幂等性的接口