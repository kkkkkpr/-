### 哪些问题适用消息队列

可以进行异步任务的消息传递，同时可以缓存消息解决生产与消费能力不一样的问题。

- 适用场景：

#### 1. 异步处理：

- - - **场景**：即某些**无法通过水平扩容**来解决的场景，如**本身就非常耗时的任务**（如训练模型、视频处理等）增加了实例节点也没用，同步处理还是会等待很长时间；**限制性资源**，无法通过增加节点解决，如数据库瓶颈；**一个业务流程，**每个业务单独处理时间都不长但是加起来就挺多的，且扩容也没用。以上无法同步长时间等待结果，异步与解耦的区别在于，**解耦是服务本身就不相互依赖，但是异步可能还是关注结果，只是可以回头再处理**。
    - 如**秒杀系统的多个业务流程**，核心是**利用有限的服务器资源，尽可能多的处理短时间内海量的请求**。如秒杀中包含了很多步骤（库存锁定、生成订单、短信通知、更新统计数据等），若没有异步处理那么整个请求会依次处理完这4个步骤，再返回响应给用户，其核心可能只有库存锁定、生成订单这两步，这两步处理完就可以返回用户秒杀结果了，剩下的步骤并不一定要在秒杀的请求中处理，可以之后再处理。因此可以把其余步骤的请求数据放到消息队列中，由消息队列异步的进行后续操作。将秒杀请求从4步减少到2步，**响应更快**，且秒杀期间可以把服务器主要资源用来处理这2步，**提升系统并发性能**。之后再处理后续步骤。

#### 2. 流量控制（突发流量的削峰）

- - - 秒杀系统中虽然用消息队列实现了异步处理，提高系统并发性能，但如何**避免过多请求压垮系统**？因此使用消息队列隔离网关与后端服务（或者两个承压能力不同的服务），以达到流量控制和保护后端服务。
      加入消息队列后，整个秒杀流程变为：
    - 网关在收到请求后，将请求放入请求消息队列；后端服务从请求消息队列中获取 APP 请求，完成后续秒杀处理过程，然后返回结果。
    - 请求到来时被网关堆积在消息队列中，**后端服务按照自己的最大能力消费请求**。对于超时的请求可以直接丢弃。但是同时也增加了请求的调用环节，**导致总体响应时间延长**，上下游系统的同步调用都变成了异步调用，**增加系统难度**

#### 3. 解耦：

- - - **消息队列可以使得两个互不依赖的服务解耦，**即A不关心B的事情**，如发送消息给用户；或在某些工作流中**
    - 好处：

- - - - **缩短响应时间**，原本服务A要依赖等待服务B做完后才能响应，现在服务A只需要把数据丢给消息队列就可以响应了
      - **增加可靠性，**即使后续服务B挂了也不会影响到A的处理流程，服务A可以正常响应

#### 4. 消息分发

- - - 一个服务A同时要发消息给服务B、C、D，如订单系统中当一个订单生成时，后续有很多的服务需要实时获得订单数据（风控，客服，数据分析系统），若新增了服务那么需要对订单系统进行修改，这几乎是不可接受的，因此引入消息队列解决系统耦合过于紧密的问题，**适用于多个消费者共同关注同一条消息的场景**。使用消息队列在订单变化时发送订单数据到消息队列的一个主题中，下游的所有服务都订阅该主题的数据，这样所有下游系统都可以获得实时的订饭数据，而无需更改订单服务的代码，实现订单服务于下游服务的解耦

### 选型

如秒杀等超大吞吐量则不需要消息回溯等，选择kafka或rocketMQ

如公司中台对外提供能力，需要很大的接入量，考虑主题量更多的rocketMQ或者百万级别的rabbitmq

如金融业务，考虑稳定性，分布式部署的kafka和rocketmq更有优势

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723479274178-922008f7-11bd-4812-8636-12786ee344c4.png)

主从：主从是一种单点控制的架构，一个主节点与多个从节点，**主节点负责处理所有的写操作与计算，从节点负责数据备份或者读操作**。如果主节点崩溃了那么可以从从节点中选一个作为主节点。

- 进行读写分离，减轻主节点压力。
- 主节点将数据同步到从节点，保证数据一致性
- 用于：数据库读写分离，消息队列主节点收发消息，从节点用于数据冗余与负载均衡

集群：（主从是集群中的一种）**同一个服务部署在多个节点上**，**每个节点的服务之间相互对等**共同提供服务（节点可以是对称也可以是非对称的）。节点之间可以相互备份，当某个节点故障其他节点可以接管其任务，提高系统可用性

- 特点：负载均衡、高可用
- 场景：Web服务器集群（如Nginx集群提供可用性）

分布式：**系统的多个服务拆分，分布在不同的节点上，分布式系统每个节点都可以做集群，每个集群可以有自己的主从结构**

- 特点：提高性能、提高可用性、方便部署与维护
- 场景：微服务（将系统分为多个微服务，每个微服务单独部署和运行，彼此通过API或者消息队列通信），分布式文件系统

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723652636965-e0fc63cc-c22d-45e3-9c20-99c5a1669a4b.png)

### 架构

#### 总体

**kafka为C/S结构**，kafka看为三层：生产者、中转者、消费者。生产者创建消息并以特定的主题将其发送到kafka服务器。中转者会持久化存储信息而不是消费完消息就没了。消费者订阅主题并消费其中的消息，从中转者中拉取消息进行处理。

消费者：消费者主动轮询（while+sleep）从kafka拉取消息，每次拉取的消息数量根据配置可以获取不定数量的消息。

- 消费者组：Kafka 中的消费者通常是作为消费者组的一部分来工作的，**同一个消费者组中的消费者共同消费某一个或者多个主题**。同一个消费者组中的不同消费者会分配到不同的分区，以确保每个分区的数据只被同一个组内的一个消费者处理，**即每个消息只会被消费者组中的某一个消费者消费**。当一个新消费者加入或离开消费者组时，Kafka 会自动触发重新平衡（rebalance），重新分配分区给各个消费者。多个不同的消费者组互不影响，即使订阅了同一个主题， **每个消费者组有自己独立的偏移量（Offset）管理，因此同一个主题的消息可以被不同的消费者组独立消费。**  
- Broker：节点，运行kafka的服务器
- Topic：主题，**是对消息分类的方式，**生产者发送消息到特定的主题，消费者订阅主题来接受消息，一个主题的消息也可以分为多个partition分区，以实现消息的**并行处理与负载均衡**，数据存储在partition级别。
- partiotion：是topic的存储单位，每个分区都是有序、不可变的消息序列，主题名称-分析编号（如TopicA-1即为主题A的1号分区），**同时每个分区可有多个副本，提高可用性**

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723739019352-633f0d6b-6603-456e-8bc0-b4cc53e13584.png)

#### Topic

**相同业务的消息可以放到同一个主题（**如秒杀主题、短信主题**），通过Topic实现业务隔离。**

**命令行操作topic**

- 创建topic `./kafka-topics.sh --create --bootstrap-server localhost:9092 --topic niugetest`
- 查看topic `./kafka-topics.sh --list --bootstrap-server localhost:9092 `
- 查看某个topic信息 `./kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic niugetest`
- 删除topic `./kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic niugetest`

**代码操作topic**

使用Java的kafka SDK，不用提前创建主题，直接向某个主题发送消息就自动创建主题了。

topic存在哪里：**topic是逻辑上的消息分类，但是实际上物理上是按照topic的partition分片来存储在kafka broker中。**

#### partition分片

topic虽然根据业务类型对消息进行了主题分类，但是某个主题的消息依旧可能很多，因此将topic进一步分治为partition，topic在物理上实际按照partition存储在不同的kafka broker中，**同一个partition中的消息是有序的，但是同一个topic下的不同partition之间的消息是无序的**

- 分片好处：

- - 消息并行处理：数据分布在多个broker上，运行消息的并行处理，提高系统的吞吐量
  - 提高可用性：分片可以有多个副本，以提高系统的可用性
  - 消费者负载均衡：消息分布在多个broker中，避免某个broker过载。若某个主题的消息有多个消费者且消息只会被消费一次，可能 Kafka 会将这些分区分配给不同的消费者，以实现负载均衡
  - 方便水平扩展： **通过增加分区数量，可以水平扩展 Kafka 主题的吞吐量。** 即同一个主题的消息可以给更多的消费者并行消费

- 创建分片

- - `./kafka-topics.sh --create--bootstrap-server localhost:9092 --topic niugetest  --partition 3`为主题niugetest创建3个分片。可以在运行时改变分区数量以提高吞吐量，但是只能调大不能调小

- **生产者消息流入哪个partition分片？**

- - 生产者可以指定消息流入的partition，但是业务并不需要感知partition（因为生产与消费都是以主题未单位的），所以不建议指定发送到哪个partition
  - 生产者没有指定具体的partition，但是可以指定一个key（发送消息的函数的参数），用来保证改相同key的消息都在同一个partition中，**用来保证某些消息的有序性（如先到先得的抢购业务）**
  - 没有指定partition或key时，就会把消息轮流分配给每个partition，即没有顺序要求（如发送短信业务）

#### 节点Broker

 partition存放在broker中，broker是kafka的服务器节点

- broker功能：

- - 接受客户端的连接、读写请求
  - 支持客户端查询kafka集群的信息
  - 存储消息

- broker与partition的关系：**一个partition对应一个broker，一个broker可以有多个partition**，**partition存放在哪个broker是随机的。**
- **客户端如何连接集群**？：kafka采用方式三

- - 方式一：使用代理，代理与众broker打交道
  - 方式二：重定向，访问集群中任意一台，如果不是目标节点，会告诉客户端正确的节点是哪一台，**如Redis集群**
  - 方式三：客户端查询完整的路由表，根据路由表决定访问的broker。前提是broker之间相互知晓（**即每个broker都有整个集群的信息**），访问其中任意一台broker，得到所有broker的路由信息，连接到具体的broker

#### 生产者

**生产者一般是一个集成了kafka客户端的后端服务**，发送消息给kafka服务端，最终消息落到某个主题的某个分片。

如收到一个路径为`/decoupling_with_mq`的http请求就向kafka的`tp-mq-decoupling`主题中发送一条消息

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723876676853-59198c4c-9589-42d8-886f-d7e6ebdd89d5.png)

- 生产消息的流程

- - 构建kafka消息结构：

- - - ![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723877374720-eb1fb250-476d-4279-a28d-50a9abb21e58.png)
    - 相同key的消息放到同一个分区
    - value为消息的具体内容，需要被序列化为二进制、
    - header 为传递一些自定义的k-v，如traceID等
    - partition+offset为空值，传递到kafka服务端后会被写入具体的分区与偏移量，用topic+partition+offset就可以对应唯一的一条消息

- - 消息序列化为二进制结构，方便网络传输：
  - 分区选择，发送到partition分区对应的broker：

有三种发送方式

- - - 同步发送：会阻塞等待kafka服务端的响应，准确知道消息是否发送成功并处理异常，适用于对数据一致性要求较高的场景
    - 发送即忘：发送后不用等待kafka服务端的响应，也不用处理异常，适用于不需要对发送结果进行处理，对性能要求高的场景
    - 异步发送：使用回调函数来处理发送结果，不会阻塞线程来等待结果。保证系统性能的同时，满足消息传递可靠性的要求
    - ![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723907630425-90b69b0d-147a-4096-9297-8964b71594ec.png)

- - **生产者根据key或者不指定key决定消息发送到哪个分区partition，在broker创建主题时broker随机决定分区partition发送到哪个broker，再根据路由表找到分区partition所在的broker，将消息发送给broker**

#### 消费者 

重点：消费频率、消息提交、消费组

**消费者也是一个集成了kafka客户端的后端服务，通过接口向broker去拉取消息消费**

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723880832283-4a8785de-a238-4c06-b5fe-0f4e9668175a.png)

不同消费者可以同一时间消费同一个主题

同一消费者可以同一时间消费同一个主题的不同分片，但是无法保证消息的有序性

如果同一个消费者只消费一个分片就可以保证消息的有序性

**默认情况小消费者只消费连接后新产生的消息，若要消费历史消息需要传递参数指定**

- 消息的消费是**消费者主动发消息到kafka的broker来拉取消息**，而不是kafka主动推送，使得消费者可以按照自己的处理能力来消费，可以通过调节`max.poll.records`来**控制每次poll拉取消息返回的消息个数。**
- **offset**：每个消息在kafka中都有partitionID与offset，可以以此定位到消息，**消费者组消费了消息后会提交该消费者组在某个partition中的offset，下次从该offset开始消费**
- **自动与手动提交offset**

如果offset被提交，相当于该offset之前的消息都被确认了，下次从该offset的下一条消息开始消费

- - 自动提交可能导致消息没有被正确处理就提交offset，导致无法再次拉到该消息
  - 因此手动提交offset可以保证本条消息被正确处理后再向broker提交

- 消费者组：消费者组中的消费者协同消费同一个Topic或多个topic，**消费者组中的每个消费者会分配到不同的分区（同一个消费者组中每个分片只分配给一个消费者）**，消息只会被消费者组中的某一个消费者消费，消费者组用唯一groupID标识，**作用是可以水平扩展消息的消费能力。当消费者组变动时，kafka会使用rebalance再平衡自动为组内消费者分配partition分区（即用户不用也不应当关心消费的是哪个分区）**
- 消费者分配partition的策略：通过消费端的`partition.assignment.strategy`配置设置分区分配策略，可以按照范围分配，轮询分配，优先保持当前分配状态，未变动的消费者可以继续消费主题等

#### 消费者组再平衡

kafka用于管理分配partition分区给消费者组中的消费者，**保证数据负载在消费者之间均匀分布，并在消费者组新增或减少消费者时自动调整分区的分配**

- 触发时机：消费者增加或减少、主题的分区数量变化
- 再平衡步骤：暂停消费、触发再平衡（消费者组协调器group coordinate运行在kafka集群中的一个broker中）、重新分配分区、通知消费者、恢复消费
- 再平衡策略：通过消费端的`partition.assignment.strategy`配置设置分区分配策略，可以按照范围分配，轮询分配，优先保持当前分配状态，未变动的消费者可以继续消费主题等。分为需要暂停消费（时间短）和不暂停消费（减少对分配关系的变动）两种
- **消费者组协调器group coordinate**：每个消费者组都会有一个协调器，**负责管理组内的消费者与offset偏移量**

- - 消费者组管理

- - - 每个broker启动时都会有一个消费者组协调器group coordinate，每个消费者组有自己的groupID，根据groupID的hash值来决定该消费者组由哪个broker的协调器管理
    - ![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723887859772-24fc4d32-e47f-42e6-9957-6a4520134539.png)

- - 偏移量管理

- - - 协调器会将消费者组的offset存在kafka内部主题`__consumer_offsets`中，当消费者重启后就可以找到之前消费到哪里了

- 再平衡影响：

- - 导致重复消费：1、当某个消费者消费完消息后还没来得及提交offset就掉线了，那么该分区重新分配的消费者就会重复消费信息  2、再平衡的过程中会暂停消费。 因此避免误判消费者掉线，通过参数`session.timeout.ms`调大超时时间为5s或10s，`heartbeat.interval.ms`心跳时间，设置为超时时间内至少可以3-5次心跳机会的时间。

### 实践经验

在使用过程中会遇到消息丢失、消息重复消费、如何保证消息有序、消息积压等问题

#### 使用

在linux中使用docker安装kafka容器，用`docker exec -it kafka-test /bin/bash`进入到容器中，kafka中有许多的bash脚本在`/opt/kafka/bin`目录中，可以用于快速的生产任务、消费任务、查询主题、修改主题等。即使不用继承java的库也可以操作kafka，脚本是一系列命令的集合，可以向脚本中传递命令行参数![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723740413998-68578b4c-be95-4be3-a537-a4df547f96e2.png)

- ` sh kafka-conslie-producer.sh --broker-list localhost:9092 --topic test`是用kafka-conslie-producer.sh从命令行向kafka主题发生消息工具（相当于一个本地生产者）向broker地址和端口localhost:9092的test主题发送消息
-  `./kafka-topic.sh --create --bootstrap-server localhost:9092 --topic niugettest`使用脚本在运行于 `localhost:9092` 的 Kafka broker 上创建一个名为 `niugettest` 的新主题。   

####  消费语意  

-  kafka的消费语意的分类：at most once最多一次（但可能丢失）、at least once至少一次（可能重复消费）、exactly once精确一次（需要业务用幂等配合）
- at most once: 将`enable.auto.commit`设置为true即自动提交，`**auto.commit.interval.ms** `设置为一个较低的时间范围，即会按照该时间间隔自动提交offset
- at least once：将`enable.auto.commit`设置为false即手动提交，后端每次消息处理完之后手动调用函数 **consumer.commitSync()** 异步提交 **offset**
- **exactly once：**将`enable.auto.commit`设置为false即手动提交**，并通过业务逻辑实现精确一次消费，**即在后端以**事务的方式保存处理结果与提交偏移量操作**，防止处理成功了但没有提交偏移量或提交成功确没有处理成功，同时用幂等操作防止重复处理 （at least once +幂等操作即为exactly once）

#### 如何保证消息不丢失

即需要实现at least once消费语意。消息可能在生产、存储、消费阶段丢失。

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723976868554-8c5b2c5a-e7a6-4d00-961a-9633536d7550.png)

- 生产环节：生产者在发送消息到kafka broker时使用同步发送或者异步发送（**保证收到发送成功的响应**）否则就不断**重试**。但是可能生产者可能重试的时候挂掉了，因此生产者每发完一个消息需要记录发送到哪里了，保证不会漏发，虽然可能会导致重复发送，但是可以在消费端的业务用幂等等方式保证不重复消费
- 存储环节：kafka消息队列会将消息持久化存储。生产者在发送消息给kafka时，kafka的**消息确认机制保证kafka服务器收到消息**

- - kafka有**消息确认机制acks**，默认为1

- - - 设置为0时kafka服务器不会返回确认响应。
    - 设置为1时kafka集群的leader节点确认消息后返回响应。
    - 设置为-1或all时kafka集群的所有副本都确认消息后会返回响应。

- 消费环节：kafka有offset偏移量来管理消息的消费位置。可以通过实现消息成功处理后再**手动提交消息确认**的方式保证消息至少被消费一次的消费语意

#### 如何保证消息不被重复消费

- **消息重复消费的场景**：生产阶段消息被重复发送了，消费阶段的业务在收到消息后没有即使确认提交offset又拉到了相同的消息。
- 解决方式：**实际上没法保证不会拉到重复的消息，因此使用幂等操作保证相同的消息不会重复产生影响来支持可重入消费**

- 幂等：即同样的操作无论执行了多少次，结果都是一样的，即只有第一次执行才产生影响。**幂等性消费与kafka无关，主要是通过业务手段**

- - **redis幂等处理：**1、每个消息分配一个全局唯一的标识符（UUID）2、Redis set将已经消费的消息ID存入set结构中，每次消费消息前先检查该消息ID是否已经存在 3、使用原子操作`SISMEMBER`和`SADD` 检查与添加
  - **mysql幂等处理：**1、为消息ID创建UNIQUE唯一约束（当重复的消息ID插入时会报错）2、使用事务来确保多个操作的原子性 3、使用mysql提供的` INSERT ... ON DUPLICATE KEY UPDATE ...`语句检查重复数据。
  - redis与mysql结合：在进入mysql之前，先在redis过滤一遍，在redis中缓存已经处理过的唯一key，如果redis中不存在，再进入mysql进行幂等操作。

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1723982167646-82c11588-9f6c-439c-aa86-2fcea0bb7e1a.png)

#### 如何让消息有序

topic从全局上看做是无序的，但是其每个分区都是有序的。

- **根据业务来分区**：

- - 将某个业务的所有消息都使用key放到同一个分区中，如秒杀业务的消息都放到某个topic的同一个key的分区下

- - - 缺点：所有消息都放到同一个partition下，降低了扩展性，无法发挥多partition的优势。

- 业务内再次分区：只根据业务分区可能会导致单个业务压力过大，

- - 可以在**将业务进一步按子业务细分**（如金融业务再拆分为风控子业务，支付子业务等）
  - 如果对于客户而言数据是比较独立的，可以**按照客户hash来区分**，如按照客户ID的hash值取模来区分partition

- - - 缺点：如果partition分区的数量变动了，那么会对原本的路由分布造成混乱。可以采用**redis哈希槽**或者**一致性哈希,**减少路由分布的变动。

- - - - redis哈希槽：redis集群使用哈希槽来管理数据分片，即引入槽的概念，分为了很多个槽位，不同的请求按照id的hash值取模分流到不同的槽位，每个节点（分区）负责一部分的槽，id对应到哪个槽，就与负责该槽的分区交互，每个节点使用bitmap来标识自己负责的槽位，当分区节点变化时，可以将部分槽位分给新的节点，而不用将全部分区都打乱。

#### 消息积压如何处理

- （**提前避免**）发生前尽量避免：

- - 提前做压测，了解业务的消费能力，合理部署业务资源，与消息的生产速度适配
  - 做好优化，提升业务的吞吐能力
  - 在大流量来临前做好资源的扩容

- (**及时发现**)积压发生时能即使发现：监控告警

- - 如通过定时服务检测kafka的队列信息，查看某个消费者组的消费offset与当前最新消息的offset

- （**异常**）异常消息导致的阻塞：

- - 若是顺序消费的场景，由于at least once的消费语意消息至少需要被消费一次，若某条消息有问题，则整个消息队列都会阻塞无法更新offset，需要升级消费端代码，处理掉该消息

- （**拖后腿**）非核心模块拉低消费速度：如秒杀中有个数据统计上报的业务，当需要更高吞吐能力时可以将该业务关掉（业务降级）
- （**单纯的扛不住压力**）：

- - 资源扩容，提高消费能力
  - 保新，如果业务允许，将新的消息导入到新的消息队列，旧的消息等之后再处理。

### kafka高可用

#### 多副本机制

kafka通过多副本机制实现容灾

- 优势

- - 高可用：如果leader副本所在的broker宕机，**kafka会从其他副本中选一个新的leader**
  - 容灾：即使部分副本数据丢失，只要有一个副本是完整的，数据就不会丢失，数据是多备份的
  - 提高读写性能：默认情况下副本只用来备份消息，但是如果业务需要，也可以让消费者从follower上消费数据。

- 副本概念

- - replica：指集群中的副本（包括leader与follower），**每个分区中有多个副本**，每个副本都保存了分区的完整数据
  - leader：负责处理该集群的所有读写请求，并向该分区中的其他副本同步数据，保证数据的一致性
  - follower：从leader中同步数据，在leader副本挂掉后，会从follower中选举一个新的leader从而保证可用性

- 创建多副本

- - **副本个数在主题创建的时候被指定，**若副本数量指定为1那么即只有一个leader副本，没有其他follower副本备份，生产环境一般设置3个副本实现容灾
  - 命令行语句`./kafka-topic.sh --create --bootstrap-server localhost:9092 --topic niutest --partition 2 --replication-factor 2`即在主题niutest下有2个分区，每个分区有2个副本，其中**分区partiton数量必须小于broker数量**

#### 多副本下的生产者写入机制

- 分区的多个副本中有leader副本，生产者只和leader打交道，leader再与其他的副本进行数据同步，根据用户配置的acks确认机制，为0则leader不回包，为1则leader确认收到后回包，-1则leader等待**所有ISR副本都确认**后再回包

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1724056855088-23953a67-ebd5-43d2-b4f7-7b2e1ed52ebf.png)

- ISR机制

- - ISR（in sync replicas)：指与leader副本保持同步的副本集合。即与leader数据差异在一定阈值内的follower。
  - 每个partiton的leader都会动态的维护一个与自己保持同步的ISR列表。若follower副本比leader副本落后了默认为10s的阈值（即当前时间与上一次follower副本完全跟上的时间的差）

#### 副本同步机制

**follower主动从leader拉取数据进行数据同步。**

根据一个偏移来拉数据，每个副本都维护一个同步的偏移量LEO（log end offset即下一个要写入的位置），在所有ISR副本都同步过的数据才算真正的落地，使用HW（high watermark）水位来标记所有副本的同步进度（每个副本保存HW），在HW以下的消息都是所有副本都同步过的，高于HW的消息说明还没有被所有副本都复制，同步的过程即follower追赶leader的LEO的过程。

#### 同步机制优化

![img](https://cdn.nlark.com/yuque/0/2024/png/46064027/1724062052965-01b951ee-39f5-4077-b3f0-f03cf7c5b2ec.png)

由HW引起的数据丢失：leader的HW先更新，再同步给follower的HW，二者之间存在时间差，在follower的HW还没有来的即更新之前follower挂了，重启后follower会截断自己当前HW之后的消息，再去leader中同步数据，此时如果leader也重启了，那么follower就变成了leader，原本截断后的数据就没有了，即可能会造成数据丢失。

引入epoch用来保证当一个新的leader被选取出来后，不会丢失上一任leader锁提交的所有数据，通过比较epoch来实现。epoch记录了每任leader数据的起始偏移。